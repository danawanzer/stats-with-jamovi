## Alpha & p-values

*If you'd like to read a more in-depth discussion of p-values, I recommend also reading [Daniel Lakens' chapter in his textbook "Improving Your Statistical Inferences."](https://lakens.github.io/statistical_inferences/pvalue.html)*

Whereas effect sizes tell us about *practical* significance, they do not tell us about *statistical* significance. That is what *p*-values are for: they tell us whether our results are *statistically* significant or how surprising they are. The formal definition of a ***p*****-value** is that it is the probability of observing data that is as extreme or more extreme than the data you have observed, assuming the null hypothesis is true. There's two things to keep in mind about this definition:

1.  The p-value is about the **probability of our [data]{.underline}**. It is not about the probability of our hypothesis.
2.  The p-value is **based on the assumption that the [null]{.underline} hypothesis is [true]{.underline}**. In null hypothesis significance testing, we are only ever testing against the null. We can never "accept" the alternative hypothesis but rather fail to reject the null. If we reject the null enough times (and rarely fail to reject the null) then it gives weighted evidence towards our alternative hypothesis, but we can never prove the alternative hypothesis is true.

Another way we could think of the *p*-value is: assuming there is no difference (i.e., the null hypothesis is true), how surprising is our data?

You may have heard of *p*-values before. You may have heard about them in a previous statistics course or you may have heard of them in relation to the "replication crisis." You may have heard about the journal that banned *p*-values altogether. You may then be wondering why we are learning about *p*-values if they seem so problematic that they should be banned.

The biggest problem with *p*-values is that they are misunderstood, even by researchers. They are often misinterpreted. Daniel Lakens has a great [blog post](http://daniellakens.blogspot.com/2017/12/understanding-common-misconceptions.html) on the topic and a great [MOOC](https://www.coursera.org/learn/statistical-inferences) about improving your statistical inferences. One of your assignments is from that MOOC called "Understanding common misconceptions about *p*-values." Whereas that assignment focused on understanding misconceptions, I will spend this chapter talking about what *p*-values *are.*

### Alpha

The *p*-value is the probability of the "surprisingness" of your data. We've already seen this chart from the last chapter where the area in red is our alpha region.

The alpha level is simply the level at which we consider the data *so surprising* that we reject the null hypothesis, and that level is the alpha level. Everything in the red region is our alpha and signifies the region of statistical significance. Therefore, whether a *p*-value is statistically significant depends on what our alpha is set at.

```{r echo = FALSE, warning=FALSE, message=FALSE, fig.cap="Critical area of statistical significance"}
library(tidyverse)
library(viridis)

ggplot(data.frame(x = c(-3.3, 3.3)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),
                xlim = c(1.645, 3.2), geom = "area", fill = "darkred") +
  geom_vline(xintercept = 1.645, colour = "darkred") + 
  geom_text(aes(x = 1.8, y = .25, label = "Critical value (alpha)"), 
            angle = 90, color = "darkred") +
  geom_vline(xintercept = 0, colour = "black") +
  geom_text(aes(x = .15, y = .25, label = "Mean difference = 0"),
            angle = 90, color = "black") + 
  # geom_text(aes(x = 8.8, y = .15, label = "Calculated-value = 8.44"), 
  #           angle = 90) +
  # geom_vline(xintercept = 8.44) + 
  theme_void() +
  scale_color_viridis(discrete = TRUE) +
  scale_y_continuous(expand = c(0,0)) +
  scale_x_continuous(limits = c(-3.5, 3.5)) + 
  ylab(element_blank()) +
  xlab(element_blank())
```

Why is alpha usually set at 5%? It comes from Fisher (1925), who said something that eventually grew to tradition:

> A deviation exceeding the standard deviation occurs about once in three trials. Twice the standard deviation is exceeded only about once in 22 trials, thrice the standard deviation only once in 370 trials.... The value for which P = .05, or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not. Deviations exceeding twice the standard deviation are thus formally regarded as significant.

Basically, .05 was convenient. It was 1/20. It was around 2 standard deviations from the mean in a normal distribution. For some reason, it caught on (maybe the "formally regarded as significant" was why).

However, a year later, even Fisher acknowledged we shouldn't just arbitrarily use *p* = .05 as our alpha level.[^05.2-p-values-1] Rather, we should consider setting it at higher odds (e.g., *p* = .01). He also argued, "A scientific fact should be regarded as experimentally established only if a properly designed experiment *rarely fails* to give this level of significance." (Fisher, 1926, p. 504).

[^05.2-p-values-1]: If you want to read more, this is a short read on the history of the .05 alpha level: <https://www2.psych.ubc.ca/~schaller/528Readings/CowlesDavis1982.pdf>

In other words, we need to think critically about the alpha level we set *and* we need to test an effect multiple times before we start thinking the alternative hypothesis is true. We'll discuss power next before we start putting it all together.

[![Courtesy of Dr. Jess Hartnett (\@Notawful on Twitter)](https://pbs.twimg.com/media/FdXMPMhX0AIj6qb?format=jpg)](https://twitter.com/Notawful/status/1573388208314159104)

### Are p-values bad?

Some have argued that we should abandon the *p*-value; this has led to things like journals completely banning *p*-values altogether. However, I agree with Lakens that ["the practical alternative to the *p*-value is the correctly used *p*-value."](https://psyarxiv.com/shm8v/) That's to say: there is nothing *wrong* with the *p*-value inherently, and it can be useful. Rather, what's *wrong* is that many people use them incorrectly.

### Video

The following video walks through some of the effect sizes, alpha, and power stuff.

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(vembedr)
embed_url("https://www.youtube.com/watch?v=v1GrXJmUkBw")
```

You might also like this video by Daniel Lakens and how he defines the p-value.

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(vembedr)
embed_url("https://www.youtube.com/watch?v=RVxHlsIw_Do")
```

*If you'd like to read a more in-depth discussion of p-values, I recommend also reading [Daniel Lakens' chapter in his textbook "Improving Your Statistical Inferences."](https://lakens.github.io/statistical_inferences/pvalue.html)*
