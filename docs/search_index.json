[["regression.html", "10.2 Regression", " 10.2 Regression 10.2.1 Overview Regression can examine multiple predictor variables simultaneously. Whereas the factorial ANOVA can only handle categorical variables (i.e., nominal or ordinal), regression can handle all types of predictor variables including both categorical and continuous. There are three types of regression in general: Linear regression: this looks at the effect of a single predictor (IV) on a single outcome (DV). This is equivalent to a t-test (dichotomous predictor), one-way ANOVA (ordinal predictor), or correlation (scale predictor). Multiple regression: this looks at the effect of multiple predictors (IVs) on a single outcome (DV). Hierarchical regression: this looks at the effect of multiple predictors (IVs) on a single outcome (DV), but there are multiple blocks or steps so that you can check the added predictability of new variables. Note that the linear regression is actually equivalent to a lot of the statistics weve learned. For example, the linear regression will produce the same results as a t-test when we have a dichotomous predictor, a one-way ANOVA when we have an ordinal predictor, and a correlation when we have a continuous predictor. Well learn more about this at the end of the textbook when we wrap everything up. 10.2.1.1 Understanding regression A linear regression model is basically a linear line, which many of us learned as y = mx + b, where y is our predicted outcome score, x is the IV, b is the intercept (the score in y when x = 0), and m is the slope (when you increase x-value by 1 unit, the y-value goes up by m units). Lets imagine we have a dataset of dragons with a categorical predictor (whether they are spotted or striped) and a continuous predictor (height) and a continuous dependent variable (weight). We want to use this dataset to be able to predict the weight of future dragons. First, lets learn how to interpret the coefficients for our two predictor variables (images from Allison Horst). Figure 10.1: Regression lines and residuals We determine our line equation from the scatterplot of scores by figuring out the line that fits closest to all data points. The regression line is the line with the smallest residuals between the line and data points. Lets visualize the regression line for how Dans sleepiness affect Dans grumpiness. On the left, we see the regression line (in purple) is very close to the data points and the residuals (the grey lines between the purple line and the data points) are smaller. On the right, we see the regression line is far from a lot of the data points and the residuals are larger. Figure 10.2: Regression lines and residuals Lets go back to our dragon example and input one of our dragons into the model to find out how residuals work. On the left, based on our dataset and the fact that our dragon is striped (spotted = 0) and has a height of 5.1 feet, we would expect our dragon to weigh 3.9 tons. However, when we actually weigh him, he weighs 4.2 tons! Therefore, the residual in this case would be .3 tons. Figure 10.3: Regression lines and residuals One of our assumption checks is that our residuals are normally distributed, so we would take all our residuals and examine those for normality. Figure 10.4: Regression lines and residuals There is more math to regression, which is needed to calculate the F-test you get for the overall model test and the t-tests you get for your model coefficients, but we wont get into that detail. 10.2.2 Look at the data Lets run an example with data from lsj-data. Open data from your Data Library in lsj-data. Select and open parenthood. This dataset includes the sleep quality of both Dan and Dans baby, Dans grumpiness, and the day of the data collection from 1-100. 10.2.2.1 Data set-up Our data set-up for regression depends on the type of regression and type of data, but in general well have one column of our continuous DV and one or more columns of our IV(s). For this chapter, were going to return to the parenthood dataset from lsj-data. Remember that this dataset includes the sleep quality of both Dan and Dans baby, Dans grumpiness, and the day of the data collection from 1-100. 10.2.2.2 Describe the data Once we confirm our data is setup correctly in jamovi, we should look at our data using descriptive statistics and graphs. First, our descriptive statistics are shown below. We can see first that we have 100 cases and no missing data. The means, medians, standard deviations, and variances are then shown, followed by the minimum and maximum values. We also see skew and kurtosis. Calculating the z-score for all the skew and kurtosis (remember: skew or kurtosis divided by its standard error) suggests we do not violate the assumption of normality much except for day. However, notice what the variable day is! Its just the day of the study, from 1-100. If you look at the graph, it has a uniform distribution (completely flat and uniform) not a normal distribution (bell curve)! 10.2.3 Check Assumptions 10.2.3.1 Assumptions The regression has a lot of assumptions. Some require no testing: Variable types: The DV is continuous and the IVs are either continuous, dichotomous, or ordinal. Independence: All the outcome variable values are independent (e.g., come from a separate entity). Other assumptions require testing: No outliers: There shouldnt be any data in the dataset that is an outlier which would strongly influence your results. Normality of the residuals: Up to this point, weve examined the normality of the outcome variables. With regression, our variables can be non-normal as long as the residuals (i.e., error) are normally distributed. Linearity: The relationship between each IV and DV is linear. Sometimes you may expect a curvilinear relationship between an IV and DV, in which case we square or cube the IV and use that variable as our predictor variable in the regression. Homogeneity of variance (homoscedasticity): At each level of the predictor variables, the variance of the residual terms should be constant. Independent residuals: For any two observations, the residual terms should be uncorrelated (or independent). Our errors must be normally distributed and uncorrelated. No multicollinearity: There should be no perfect or near-perfect linear relationship between two or more of the predictors in your regression model. For example, you would not include heigh_cm and heigh_in in your model because they would be perfectly related to one another. Well learn how to test for this. 10.2.3.2 Checking Assumptions 10.2.3.2.1 Outliers Under Data Summary, you should have a table with Cooks distance. This is one way we can check for multivariate outliers. This examines whether any one line of data is an outlier, not just one data point. In general, Cooks distances greater than 1 indicate a multivariate outlier. Our Cooks distances are very small, so we do not have a problem with outliers. Figure 10.5: Checking for multivariate outliers in jamovi 10.2.3.2.2 Normality of the residuals The regression analysis in jamovi allows us to check normality with the Shapiro-Wilks test and the Q-Q plot of our residuals. Weve seen this multiple times, so by now it should be well-ingrained that because Shapiro-Wilks is not statistically significant and our data points fall along the diagonal line that we satisfy the assumption of normally distributed residuals. Figure 10.6: Checking the normality of residuals in jamovi 10.2.3.2.3 Linearity &amp; homoscedasticity To examine linearity and homoscedasticity, two of the assumptions of regression, we examine the Residuals Plots. You will get one plot of the overall model (Fitted) and one for each of your variables (DV and IV(s)). Ive displayed the residuals plot for the Fitted values against the residuals below. In these plots, we want our data to look like a random scattering of dots even dispersed around zero on the y-axis. Linearity: If the data points seem to have a curve in the graph, then that suggests you have failed the assumption of linearity. Our data doesnt seem to have any curve to it, so we satisfy the assumption of linearity. Homoscedasticity: If the graph seems to funnel (e.g., widely dispersed on one end of the x-axis and narrowly dispersed on the other end), then that suggests you fail the assumption of homoscedasticity. Our data doesnt seem to be wider at any point, so we satisfy the assumption of homoscedasticity. Figure 10.7: Checking linearity and homoscedasticity residuals in jamovi 10.2.3.2.4 Independence of residuals The Durbin-Watson test for autocorrelction tests for independence of residuals. We want the Durbin-Watson value to be as close to 2 as possible. Values less than 1 or greater than 3 are problematic and indicate we are violating this assumption. In our case, the DW test statistic is 2.12 and so very close to 2. Furthermore, they provide a p-value and the p-value is greater than .05 so the test statistic is not statistically significant, further supporting that we meet the assumption that our residuals are independent. Figure 10.8: Checking independence of residuals in jamovi 10.2.3.2.5 Multicollinearity Multicollinearity is a problem for three reasons: Untrustworthy Bs: As multicollinearity increases, so do the standard errors of the B coefficient. We want smaller standard errors, so this is problematic. Limits the size of R, and therefore the size of R2, and we want to have the largest R or R2 possible, given our data. Importance of predictors: When two predictors are highly correlated, it is very hard to determine which variable is more important than the other. Multicollinearity is simply that multiple variables are correlated. We can first just look for general collinearity, or the correlations between all our predictors, using the correlation matrix in jamovi. Any correlations greater than .8 or .9 are problematic. You would either need to drop one variable or combine them into a mean composite variable. However, to test for multicollinearity, we examine the VIF and Tolerance values. VIF is actually a transformation of Tolerance (Tolerance = 1/VIF and VIF = 1/Tolerance). In general, we want values 10 or lower, which corresponds to Tolerance values greater than .2. In our data, our VIF is 1.65 and Tolerance is .61, so we satisfy the assumption of no multicollinearity. Figure 10.9: Checking multicollinearity in jamovi Now that we met all the assumptions, we can interpret our results! 10.2.4 Perform the test From the Analyses toolbar select Regression - Linear regression. Note that we select this option regardless of whether we are performing a linear regression, multiple regression, or hierarchical regression. Move your dependent variable dan.grump into the Dependent Variable box and all your independent variables into either Covariates (if they are continuous variables) or Factors (if they are categorical variables). In this case, all our variables are continuous so move both dan.sleep and baby.sleep to the Covariates box. If you are performing a hierarchical regression, you will use the Model Builder drop-down menu. More information on hierarchical regression will be discussed later. If you have categorical predictors with more than two levels, you will use the Reference Levels drop-down menu to specify what you want your reference level to be and whether you want the intercept to be the reference level or the grand man. More information on categorical predictors will be discussed later. Under Assumption Checks, check all the boxes! Under Model Fit, select R, R-squared, Adjusted R-squared, and F test. The other options (AIC, BIC, RMSE) are more useful when we are comparing models and will be discussed later in the Hierarchical regression section. Under Model Coefficients, select Standardized estimate. Optionally, you can ask for plots and tables of the estimated marginal means. Im not going to show the set-up figure here because theres just too much to show. 10.2.5 Interpret results Figure 10.10: Regression results in jamovi The first table shows us our overall model results. R, R-squared, and adjusted R-squared: We get our R and R-squared values (R-squared literally being R squared). Remember back to correlation: R-squared is the proportion of variance in the dependent variable that can be accounted for by the predictor(s). In this case, Dan and the babys sleep quality predict 82% of the variance in Dans grumpiness. However, more commonly we report the adjusted R-squared value, which adjusts the R-squared value based on the number of predictors in the model. Adding more predictors to the model will always cause R-squared to increase (or at least not decrease) so its important that we control for that using an adjustment. Its interpreted basically the same, just adjusted for biased. I encourage you to use the adjusted R-squared, especially if you have lots of predictors in your model. Overall Model Test: We also get an F-test for the overall model. If you want, you can get the full ANOVA test by selected ANOVA test under Model Coefficients. This is how we know if the overall model is statistically significant. In our case, our F-test is statistically significant so we know that the set of predictors significantly predicts our dependent variable. Model coefficients: Just like in ANOVA, we first examine if the model is significant (overall model test) and then look at individual factors, in this case being individual variables in our regression model. Each variableour intercept and both independent variableshave an associated t-test. In this case, Dans sleep significantly predicts Dans grumpiness, but the babys sleep does not. Standardized coefficients: We also asked for standardized estimates, which we get in the last column of our model coefficients table. These are standardized so that we can compare them to other variables. They give us an idea of the strength of the relationship between that IV on the DV. Larger values = bigger effects. The standardized estimate is called Beta (\\(\\beta\\)) whereas the unstandardized estimate is just called that or B (the letter B, not Beta). We use the standardized estimates to compare the strength of the estimate to other IVs and we use unstandardized estimates to write our linear equations and predict the DV given values of the IV. What about the intercept? You might be wondering what we do with the intercept. Typically, nothing. We only use it to create our equation so that we can predict Dans grumpiness based on Dans sleep and the babys sleep. For example, our equation from our data is such: \\(y = 125.97 - 8.95(dan.sleep) + .01(baby.sleep)\\) If Dans sleep was 5 and babys sleep was 8, then wed expect Dans grumpiness to be: \\(y = 125.97 - 8.95(5) + .01(8) = 125.97 - 44.75 + .08 = 81.3\\) 10.2.5.1 Write up the results in APA style We can write up our results in APA something like this: Dan collected data on how many hours of sleep Dan and Dans baby got, as well as Dans grumpiness, for 100 days. Dan tested how the hours of sleep both Dan and the baby got affected Dans grumpiness using a multiple regression. The combination of predictors were significantly related to Dans grumpiness, F (2, 97) = 215.24, p &lt; .001, adjusted \\(R^2\\) = .81. The number of hours of sleep Dan got significantly predicted Dans grumpiness, \\(\\beta\\) = -.90, t (97)= -16.17, SE = .55, p &lt; .001. However, the number of hours of sleep the baby got did not significantly relate to Dans grumpiness, \\(\\beta\\) = 0.00, t (97)= .04, SE = .27, p = .969. Note that in many of these write-ups I did not include anything about assumption checking. I normally write up that information as part of my analytic plan in my methods section (e.g., I checked for multivariate outliers using Cooks distance.). Included in this section, I explain what I will do if I do not meet various assumptions. Then, if I dont meet the assumption in the results section I explain that I did not meet the assumption, explain the results if necessary, explain what I did, and then give the results. In this case, we met all the assumptions (that presumably I described in my methods section) and therefore went straight to the results. 10.2.6 Additional information 10.2.6.1 Hierarchical regression Hierarchical regression is exactly the same as multiple regression but now we have multiple models or blocks. You can specify hierarchical regression using the Model Builder drop-down menu in jamovi. Lets try an example where we have baby.sleep as Block 1 and dan.sleep as Block 2. In addition, using the Model Fit drop-down menu you should check AIC and BIC in addition to the previously selected options. Your setup should look something like this: Figure 10.11: Hierarchical regression setup in jamovi Our model results now change. We now have two lines for the Model Fit Measures and a Model Comparisons table. In addition, under Model Specific Results, we have a drop-down menu to specify which model we want to examine. Figure 10.12: Hierarchical regression results in jamovi Lets interpret. Our first model (with just baby.sleep is significant), F (1, 98) = 46.18, p &lt; .001, adjusted \\(R^2\\) = .31. So is our second model (that has both baby.sleep and dan.sleep), F (2, 97) = 215.24, p &lt; .001, adjusted \\(R^2\\) = .81. There was a significant improvement between model 1 and model 2, \\(F_{change}\\) (1, 97) = 261.52, p &lt; .001, \\(\\Delta R^2\\) = .50. The significant improvement means that the predictors added to model 2 significantly predict our DV above and beyond the predictors in model 1. We might write-up these results as such: Dan collected data on how many hours of sleep Dan and Dans baby got, as well as Dans grumpiness, for 100 days. Dan tested how the hours of sleep both Dan and the baby got affected Dans grumpiness using hierarchical regression to find out how Dans sleep predicted Dans grumpiness above and beyond the babys sleep. First, the babys sleep significantly predicted Dans grumpiness, F (1, 98) = 46.18, p &lt; .001, adjusted \\(R^2\\) = .31. As the babys hours of sleep increased, Dans grumpiness decreased, t (98) = -6.80, SE = .40, p &lt; .001, \\(\\beta\\) = -.57. A second model was tested that added Dans sleep. This modelcomprised of both babys sleep and Dans sleepsignificantly predicted Dans grumpiness, F (2, 97) = 215.24, p &lt; .001, adjusted \\(R^2\\) = .81. There was a significant improvement between model 1 and model 2, \\(F_{change}\\) (1, 97) = 261.52, p &lt; .001, \\(\\Delta R^2\\) = .50. In the second model, as Dans sleep increased, Dans grumpiness decreased, t (97) = -16.17, SE = .55, p &lt; .001, \\(\\beta\\) = -.90. However, the babys sleep did not significantly relate to Dans grumpiness when controlling for Dans sleep, t (97) = .04, SE = .27, p = .969, \\(\\beta\\) = .00. 10.2.6.2 Categorical Predictors Dummy coded variables (with values 0 or 1) are pretty easy to interpret in regression. If the Beta is positive, then the value of 1 would have a higher mean on the DV than the value of 0. If the Beta is negative, then the value of 0 would have a higher mean on the DV than the value of 1. However, if we have a nominal variable with more than two categories, then we need to dummy code the data to analyze in a regression. Fortunately, jamovi can do this automatically for us! The dataset were using doesnt currently have a categorical variable, so Im going to manually create one for demonstration purposes. Im going to transform the day variable, which is the day of the data collection from 1 to 100, into a new variable that indicates whether the day is 1-32, 33-65, or 66-100, which is roughly 3 equal groups. You can see the transformation here: Figure 10.13: Transforming day into a categorical variable in jamovi Lets add that to our regression model and just make it a simple multiple regression model. Three independent variables (dan.sleep, baby.sleep, and our new day_3groups variable) all in one block. Now we need to go to the Reference Levels drop-down menu. We have two options: Reference level (dummy coding): We can have our intercept be the mean of our reference level group, meaning that if all other variables were set to 0 this is the mean of our dependent variable for that group. For example, if we set day = 1 to be our reference level, then the intercept is the value of Dans grumpiness when Dans sleep is 0 and babys sleep is 0 for the first 32 days. This is the option I normally choose. Grand mean (simple coding): Alternatively, we can have our intercept be the grand mean, or the overall mean when all other variables were set to 0 and we ignored day. I am not sure when I would use this option, to be honest. The other option you have is what is considered your reference level. It will default to your first level in your dataset (in this case, day_3Groups = 1) but you can change to any other level in your variable. I set my reference level to be 1, the default, and I know that because the day variable compares both levels 2 and 3 to 1. Our intercept (126.02) then is the value of grumpiness if Dan and the baby slept 0 hours in the first 32 days of our data collection. Figure 10.14: Regression with a categorical predictor results in jamovi The first line of day - 3_Groups (2  1) is then the difference in Dans grumpiness between the second 1/3 of days (days 33-65) and the first 1/3 of days (days 1-32). It is not statistically significant, so there is no difference in Dans grumpiness between the first and second 1/3 of days. Because the estimate is negative, that indicates that the first 1/3 of days have a higher estimated mean of Dans grumpiness, but again its not statistically significant. The second line of day - 3_Groups (3  1) is the difference in Dans grumpiness between the third 1/3 of days (days 66-100) and the first 1/3 of days (days 1-32). It is not statistically significant, so there is no difference in Dans grumpiness between the first and third 1/3 of days, either. In this case, the Estimated Marginal Means can be very helpful for us to interpret the model coefficients. We can get the estimated marginal means of each group on the DV at the average levels of the other two variables. Figure 10.15: Regression with a categorical predictor results in jamovi 10.2.7 Your turn! Open the Sample_Dataset_2014.xlsx file that we will be using for all Your Turn exercises. You can find the dataset here: Sample_Dataset_2014.xlsx Download To get the most out of these exercises, try to first find out the answer on your own and then use the drop-down menus to check your answer. Perform a multiple regression examining how English, Reading and Writing, as well as Gender relate to the dependent variable Math. Do you have any significant outliers? yes no Are your residuals normally distributed? yes no Do you satisfy the assumption of linearity and homoscedasticity of your residuals (just check the Fitted residual plot)? yes no Do you meet the assumption of independent residuals? yes no Do you meet the assumption of no multicollinearity? yes no Can you perform a regression with this data? yes no What is your adjusted R-squared, rounded to two decimal places: Is the overall model statistically significant? yes no Is English statistically significant? yes no Is Reading statistically significant? yes no Is Writing statistically significant? yes no Is Gender statistically significant? yes no For Gender, do male (Gender = 0) or female (Gender = 1) students have higher math scores? male female "]]
