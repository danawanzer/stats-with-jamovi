[["alpha-p-values.html", "5.2 Alpha &amp; p-values", " 5.2 Alpha &amp; p-values Whereas effect sizes tell us about practical significance, they do not tell us about statistical significance. That is what p-values are for: they tell us whether our results are statistically significant or how surprising they are. The formal definition of a p-value is that it is the probability of observing data that is as extreme or more extreme than the data you have observed, assuming the null hypothesis is true. Theres two things to keep in mind about this definition: The p-value is about the probability of our data. It is not about the probability of our hypothesis. The p-value is based on the assumption that the null hypothesis is true. In null hypothesis significance testing, we are only ever testing against the null. We can never accept the alternative hypothesis but rather fail to reject the null. If we fail to reject the null enough times (and rarely reject the null) then it gives weighted evidence towards our alternative hypothesis, but we can never prove the alternative hypothesis is true. Another way we could think of the p-value is: assuming there is no difference (i.e., the null hypothesis is true), how surprising is our data? You may have heard of p-values before. You may have heard about them in a previous statistics course or you may have heard of them in relation to the replication crisis. You may have heard about the journal that banned p-values altogether. You may then be wondering why we are learning about p-values if they seem so problematic that they should be banned. The biggest problem with p-values is that they are misunderstood, even by researchers. They are often misinterpreted. Daniel Lakens has a great blog post on the topic and a great MOOC about improving your statistical inferences. One of your assignments is from that MOOC called Understanding common misconceptions about p-values. Whereas that assignment focused on understanding misconceptions, I will spend this chapter talking about what p-values are. 5.2.1 Alpha The p-value is the probability of the surprisingness of your data. Weve already seen this chart from the last chapter where the area in red is our alpha region. The alpha level is simply the level at which we consider the data so surprising that we reject the null hypothesis. This area in red is also considered our critical test region and the region of statistical significance. Statistical significance depends on what we set our alpha at. Figure 5.1: Critical area of statistical significance Why is alpha set at 5% usually? It comes from Fisher (1925), who said something that eventually grew to tradition: A deviation exceeding the standard deviation occurs about once in three trials. Twice the standard deviation is exceeded only about once in 22 trials, thrice the standard deviation only once in 370 trials. The value for which P = .05, or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not. Deviations exceeding twice the standard deviation are thus formally regarded as significant. Basically, .05 was convenient. It was 1/20. It was around 2 standard deviations from the mean in a normal distribution. For some reason, it caught on (maybe the formally regarded as significant was why). However, a year later, even Fisher acknowledged we shouldnt just arbitrarily use p = .05 as our alpha level.1 Rather, we should consider setting it at higher odds (e.g., p = .01). He also argued, A scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails to give this level of significance. (Fisher, 1926, p.Â 504). In other words, we need to think critically about the alpha level we set and we need to test an effect multiple times before we start thinking the alternative hypothesis is true. Lets discuss power before we start putting it all together. 5.2.2 Are p-values bad? Some have argued that we should abandon the p-value; this has led to things like journals completely banning p-values altogether. However, I agree with Lakens that the practical alternative to the p-value is the correctly used p-value. Thats to say: there is nothing wrong with the p-value inherently, and it can be useful. Rather, whats wrong is that many people use them incorrectly. 5.2.3 Video The following video walks through some of the effect sizes, alpha, and power stuff. If you want to read more, this is a short read on the history of the .05 alpha level: https://www2.psych.ubc.ca/~schaller/528Readings/CowlesDavis1982.pdf "]]
