<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>21. Regression | Statistics with jamovi</title>
  <meta name="description" content="This is the website for PSYC 290 and PSYC 790 at the University of Wisconsin-Stout, taught by Dana Wanzer." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="21. Regression | Statistics with jamovi" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the website for PSYC 290 and PSYC 790 at the University of Wisconsin-Stout, taught by Dana Wanzer." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="21. Regression | Statistics with jamovi" />
  
  <meta name="twitter:description" content="This is the website for PSYC 290 and PSYC 790 at the University of Wisconsin-Stout, taught by Dana Wanzer." />
  

<meta name="author" content="Dana Wanzer" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="correlation.html"/>
<link rel="next" href="general-linear-model.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<link href="libs/vembedr-0.1.5/css/vembedr.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="include/psyteachr.css" type="text/css" />
<link rel="stylesheet" href="include/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Stats with jamovi</a></li>

<li class="divider"></li>
<li><a href="index.html#welcome" id="toc-welcome">Welcome</a></li>
<li><a href="#part-overview" id="toc-part-overview">(PART) Overview</a></li>
<li><a href="introduction.html#introduction" id="toc-introduction">1. Introduction</a></li>
<li><a href="statistics-foundations.html#statistics-foundations" id="toc-statistics-foundations">2. Statistics foundations</a>
<ul>
<li><a href="descriptive-vs-inferential-statistics.html#descriptive-vs-inferential-statistics" id="toc-descriptive-vs-inferential-statistics">Descriptive vs inferential statistics</a>
<ul>
<li><a href="descriptive-vs-inferential-statistics.html#an-example" id="toc-an-example">An example</a></li>
</ul></li>
<li><a href="central-tendency-and-dispersion.html#central-tendency-and-dispersion" id="toc-central-tendency-and-dispersion">Central tendency and dispersion</a></li>
<li><a href="levels-of-measurement.html#levels-of-measurement" id="toc-levels-of-measurement">Levels of measurement</a>
<ul>
<li><a href="levels-of-measurement.html#examples-of-levels-of-measurement" id="toc-examples-of-levels-of-measurement">Examples of levels of measurement</a></li>
</ul></li>
<li><a href="design-methods-key-terms.html#design-methods-key-terms" id="toc-design-methods-key-terms">Design &amp; Methods Key Terms</a>
<ul>
<li><a href="design-methods-key-terms.html#study-design-terms" id="toc-study-design-terms">Study design terms</a></li>
<li><a href="design-methods-key-terms.html#variables" id="toc-variables">Variables</a></li>
<li><a href="design-methods-key-terms.html#reliability-and-validity" id="toc-reliability-and-validity">Reliability and validity</a></li>
<li><a href="design-methods-key-terms.html#other-terms" id="toc-other-terms">Other terms</a></li>
</ul></li>
</ul></li>
<li><a href="#part-intro-to-jamovi" id="toc-part-intro-to-jamovi">(PART) Intro to jamovi</a></li>
<li><a href="overview-of-jamovi.html#overview-of-jamovi" id="toc-overview-of-jamovi">3. Overview of jamovi</a>
<ul>
<li><a href="describing-data.html#describing-data" id="toc-describing-data">Describing data</a>
<ul>
<li><a href="describing-data.html#data-variables" id="toc-data-variables">Data variables</a></li>
<li><a href="describing-data.html#describing-your-data" id="toc-describing-your-data">Describing your data</a></li>
<li><a href="describing-data.html#writing-up-descriptive-statistics" id="toc-writing-up-descriptive-statistics">Writing up descriptive statistics</a></li>
</ul></li>
<li><a href="visualizing-data.html#visualizing-data" id="toc-visualizing-data">Visualizing data</a>
<ul>
<li><a href="visualizing-data.html#expanding-your-data-visualization" id="toc-expanding-your-data-visualization">Expanding your data visualization</a></li>
</ul></li>
<li><a href="cleaning-data.html#cleaning-data" id="toc-cleaning-data">Cleaning data</a>
<ul>
<li><a href="cleaning-data.html#data-setup" id="toc-data-setup">Data setup</a></li>
<li><a href="cleaning-data.html#compute" id="toc-compute">Compute</a></li>
<li><a href="cleaning-data.html#transform" id="toc-transform">Transform</a></li>
<li><a href="cleaning-data.html#filters" id="toc-filters">Filters</a></li>
</ul></li>
</ul></li>
<li><a href="#part-nhst" id="toc-part-nhst">(PART) NHST</a></li>
<li><a href="hypothesis-testing.html#hypothesis-testing" id="toc-hypothesis-testing">4. Hypothesis testing</a>
<ul>
<li><a href="example-of-hypothesis-testing.html#example-of-hypothesis-testing" id="toc-example-of-hypothesis-testing">Example of hypothesis testing</a>
<ul>
<li><a href="example-of-hypothesis-testing.html#look-at-the-data" id="toc-look-at-the-data">1. Look at the data</a></li>
<li><a href="example-of-hypothesis-testing.html#check-assumptions" id="toc-check-assumptions">2. Check assumptions</a></li>
<li><a href="example-of-hypothesis-testing.html#perform-the-test" id="toc-perform-the-test">3. Perform the test</a></li>
<li><a href="example-of-hypothesis-testing.html#interpret-the-results" id="toc-interpret-the-results">4. Interpret the results</a></li>
<li><a href="example-of-hypothesis-testing.html#final-note" id="toc-final-note">Final note</a></li>
</ul></li>
</ul></li>
<li><a href="bean.html#bean" id="toc-bean">5. BEAN</a>
<ul>
<li><a href="effect-sizes.html#effect-sizes" id="toc-effect-sizes">Effect sizes</a>
<ul>
<li><a href="effect-sizes.html#small-medium-and-large-effect-sizes" id="toc-small-medium-and-large-effect-sizes">Small, medium, and large effect sizes</a></li>
</ul></li>
<li><a href="alpha-p-values.html#alpha-p-values" id="toc-alpha-p-values">Alpha &amp; p-values</a>
<ul>
<li><a href="alpha-p-values.html#alpha" id="toc-alpha">Alpha</a></li>
<li><a href="alpha-p-values.html#are-p-values-bad" id="toc-are-p-values-bad">Are p-values bad?</a></li>
<li><a href="alpha-p-values.html#video" id="toc-video">Video</a></li>
</ul></li>
<li><a href="power.html#power" id="toc-power">Power</a>
<ul>
<li><a href="power.html#alpha-power-and-error-rates" id="toc-alpha-power-and-error-rates">Alpha, power, and error rates</a></li>
<li><a href="power.html#playing-with-alpha-power" id="toc-playing-with-alpha-power">Playing with alpha &amp; power</a></li>
<li><a href="power.html#video-1" id="toc-video-1">Video</a></li>
</ul></li>
<li><a href="sample-size-power-analysis.html#sample-size-power-analysis" id="toc-sample-size-power-analysis">Sample size &amp; power analysis</a>
<ul>
<li><a href="sample-size-power-analysis.html#bean-power-analysis" id="toc-bean-power-analysis">BEAN: Power analysis</a></li>
<li><a href="sample-size-power-analysis.html#power-analysis-example-1" id="toc-power-analysis-example-1">Power analysis example #1</a></li>
<li><a href="sample-size-power-analysis.html#play-with-jpower" id="toc-play-with-jpower">Play with jpower</a></li>
</ul></li>
</ul></li>
<li><a href="inferential-statistics.html#inferential-statistics" id="toc-inferential-statistics">6. Inferential statistics</a>
<ul>
<li><a href="choosing-the-correct-test.html#choosing-the-correct-test" id="toc-choosing-the-correct-test">Choosing the correct test</a>
<ul>
<li><a href="choosing-the-correct-test.html#forward-mapping-choose-the-correct-test" id="toc-forward-mapping-choose-the-correct-test">Forward mapping: Choose the correct test</a></li>
<li><a href="choosing-the-correct-test.html#backwards-map-determine-the-data-you-need" id="toc-backwards-map-determine-the-data-you-need">Backwards map: Determine the data you need</a></li>
</ul></li>
</ul></li>
<li><a href="parametric-assumptions.html#parametric-assumptions" id="toc-parametric-assumptions">7. Parametric assumptions</a>
<ul>
<li><a href="checking-assumptions.html#checking-assumptions" id="toc-checking-assumptions">Checking assumptions</a>
<ul>
<li><a href="checking-assumptions.html#normal-distribution" id="toc-normal-distribution">Normal distribution</a></li>
<li><a href="checking-assumptions.html#intervalratio-data" id="toc-intervalratio-data">Interval/ratio data</a></li>
<li><a href="checking-assumptions.html#homogeneity-of-variance" id="toc-homogeneity-of-variance">Homogeneity of variance</a></li>
<li><a href="checking-assumptions.html#independent-scores" id="toc-independent-scores">Independent scores</a></li>
</ul></li>
<li><a href="violated-assumptions.html#violated-assumptions" id="toc-violated-assumptions">Violated assumptions</a>
<ul>
<li><a href="violated-assumptions.html#intervalratio-data-1" id="toc-intervalratio-data-1">Interval/ratio data</a></li>
<li><a href="violated-assumptions.html#independent-data" id="toc-independent-data">Independent data</a></li>
<li><a href="violated-assumptions.html#normality-or-homogeneity-of-variance" id="toc-normality-or-homogeneity-of-variance">Normality or homogeneity of variance</a></li>
</ul></li>
</ul></li>
<li><a href="writing-up-results.html#writing-up-results" id="toc-writing-up-results">8. Writing up results</a></li>
<li><a href="#part-individual-tests" id="toc-part-individual-tests">(PART) Individual Tests</a></li>
<li><a href="t-tests.html#t-tests" id="toc-t-tests">t-tests</a>
<ul>
<li><a href="one-sample-t-test.html#one-sample-t-test" id="toc-one-sample-t-test">9. One sample t-test</a>
<ul>
<li><a href="one-sample-t-test.html#step-1-look-at-the-data" id="toc-step-1-look-at-the-data">Step 1: Look at the data</a></li>
<li><a href="one-sample-t-test.html#step-2-check-assumptions" id="toc-step-2-check-assumptions">Step 2: Check assumptions</a></li>
<li><a href="one-sample-t-test.html#step-3-perform-the-test" id="toc-step-3-perform-the-test">Step 3: Perform the test</a></li>
<li><a href="one-sample-t-test.html#step-4-interpret-results" id="toc-step-4-interpret-results">Step 4: Interpret results</a></li>
<li><a href="one-sample-t-test.html#wilcoxon-w-test" id="toc-wilcoxon-w-test">Wilcoxon W test</a></li>
<li><a href="one-sample-t-test.html#your-turn" id="toc-your-turn">Your turn!</a></li>
</ul></li>
<li><a href="independent-t-test.html#independent-t-test" id="toc-independent-t-test">10. Independent t-test</a>
<ul>
<li><a href="independent-t-test.html#step-1-look-at-the-data-1" id="toc-step-1-look-at-the-data-1">Step 1: Look at the data</a></li>
<li><a href="independent-t-test.html#step-2-check-assumptions-1" id="toc-step-2-check-assumptions-1">Step 2: Check assumptions</a></li>
<li><a href="independent-t-test.html#step-3-perform-the-test-1" id="toc-step-3-perform-the-test-1">Step 3: Perform the test</a></li>
<li><a href="independent-t-test.html#step-4-interpret-results-1" id="toc-step-4-interpret-results-1">Step 4: Interpret results</a></li>
<li><a href="independent-t-test.html#welchs-t-test" id="toc-welchs-t-test">Welch’s t-test</a></li>
<li><a href="independent-t-test.html#mann-whitney-u-test" id="toc-mann-whitney-u-test">Mann-Whitney U test</a></li>
<li><a href="independent-t-test.html#additional-practice" id="toc-additional-practice">Additional practice</a></li>
</ul></li>
<li><a href="dependent-t-test.html#dependent-t-test" id="toc-dependent-t-test">11. Dependent t-test</a>
<ul>
<li><a href="dependent-t-test.html#step-1-look-at-the-data-2" id="toc-step-1-look-at-the-data-2">Step 1: Look at the data</a></li>
<li><a href="dependent-t-test.html#step-2-check-assumptions-2" id="toc-step-2-check-assumptions-2">Step 2: Check assumptions</a></li>
<li><a href="dependent-t-test.html#step-3-perform-the-test-2" id="toc-step-3-perform-the-test-2">Step 3: Perform the test</a></li>
<li><a href="dependent-t-test.html#step-4-interpret-results-2" id="toc-step-4-interpret-results-2">Step 4: Interpret results</a></li>
<li><a href="dependent-t-test.html#wilcoxon-rank" id="toc-wilcoxon-rank">Wilcoxon rank</a></li>
<li><a href="dependent-t-test.html#additional-practice-1" id="toc-additional-practice-1">Additional practice</a></li>
</ul></li>
</ul></li>
<li><a href="chi-square.html#chi-square" id="toc-chi-square">Chi-Square</a>
<ul>
<li><a href="chi-square-goodness-of-fit.html#chi-square-goodness-of-fit" id="toc-chi-square-goodness-of-fit">12. Chi-Square Goodness-of-Fit</a>
<ul>
<li><a href="chi-square-goodness-of-fit.html#step-1-look-at-the-data-3" id="toc-step-1-look-at-the-data-3">Step 1: Look at the data</a></li>
<li><a href="chi-square-goodness-of-fit.html#step-2-check-assumptions-3" id="toc-step-2-check-assumptions-3">Step 2: Check assumptions</a></li>
<li><a href="chi-square-goodness-of-fit.html#step-3-perform-the-test-3" id="toc-step-3-perform-the-test-3">Step 3: Perform the test</a></li>
<li><a href="chi-square-goodness-of-fit.html#step-4-interpret-results-3" id="toc-step-4-interpret-results-3">Step 4: Interpret results</a></li>
<li><a href="chi-square-goodness-of-fit.html#additional-practice-2" id="toc-additional-practice-2">Additional practice</a></li>
</ul></li>
<li><a href="chi-square-test-of-independence.html#chi-square-test-of-independence" id="toc-chi-square-test-of-independence">13. Chi-Square Test of Independence</a>
<ul>
<li><a href="chi-square-test-of-independence.html#step-1-look-at-the-data-4" id="toc-step-1-look-at-the-data-4">Step 1: Look at the data</a></li>
<li><a href="chi-square-test-of-independence.html#step-2-check-assumptions-4" id="toc-step-2-check-assumptions-4">Step 2: Check assumptions</a></li>
<li><a href="chi-square-test-of-independence.html#step-3-perform-the-test-4" id="toc-step-3-perform-the-test-4">Step 3: Perform the test</a></li>
<li><a href="chi-square-test-of-independence.html#step-4-interpret-results-4" id="toc-step-4-interpret-results-4">Step 4: Interpret results</a></li>
<li><a href="chi-square-test-of-independence.html#fishers-exact-test" id="toc-fishers-exact-test">Fisher’s exact test</a></li>
<li><a href="chi-square-test-of-independence.html#additional-practice-3" id="toc-additional-practice-3">Additional practice</a></li>
</ul></li>
<li><a href="mcnemars-test.html#mcnemars-test" id="toc-mcnemars-test">14. McNemar’s Test</a>
<ul>
<li><a href="mcnemars-test.html#step-1-look-at-the-data-5" id="toc-step-1-look-at-the-data-5">Step 1: Look at the data</a></li>
<li><a href="mcnemars-test.html#step-2-check-assumptions-5" id="toc-step-2-check-assumptions-5">Step 2: Check assumptions</a></li>
<li><a href="mcnemars-test.html#step-3-perform-the-test-5" id="toc-step-3-perform-the-test-5">Step 3: Perform the test</a></li>
<li><a href="mcnemars-test.html#step-4-interpret-results-5" id="toc-step-4-interpret-results-5">Step 4: Interpret results</a></li>
</ul></li>
</ul></li>
<li><a href="anova.html#anova" id="toc-anova">ANOVA</a>
<ul>
<li><a href="one-way-anova.html#one-way-anova" id="toc-one-way-anova">15. One-way ANOVA</a>
<ul>
<li><a href="one-way-anova.html#step-1-look-at-the-data-6" id="toc-step-1-look-at-the-data-6">Step 1: Look at the data</a></li>
<li><a href="one-way-anova.html#anova-assumptions" id="toc-anova-assumptions">Step 2: Check assumptions</a></li>
<li><a href="one-way-anova.html#step-3-perform-the-test-6" id="toc-step-3-perform-the-test-6">Step 3: Perform the test</a></li>
<li><a href="one-way-anova.html#step-4-interpret-results-6" id="toc-step-4-interpret-results-6">Step 4: Interpret results</a></li>
<li><a href="one-way-anova.html#welchs-f-test" id="toc-welchs-f-test">Welch’s F-test</a></li>
<li><a href="one-way-anova.html#kruskal-wallis-test" id="toc-kruskal-wallis-test">Kruskal-Wallis test</a></li>
<li><a href="one-way-anova.html#additional-practice-4" id="toc-additional-practice-4">Additional practice</a></li>
</ul></li>
<li><a href="finding-group-differences.html#finding-group-differences" id="toc-finding-group-differences">16. Finding Group Differences</a>
<ul>
<li><a href="finding-group-differences.html#post-hoc-comparisons" id="toc-post-hoc-comparisons">Post hoc comparisons</a></li>
<li><a href="finding-group-differences.html#welchs-f-test-post-hoc-tests" id="toc-welchs-f-test-post-hoc-tests">Welch’s F-test post hoc tests</a></li>
<li><a href="finding-group-differences.html#kruskal-wallis-post-hoc-tests" id="toc-kruskal-wallis-post-hoc-tests">Kruskal-Wallis post hoc tests</a></li>
<li><a href="finding-group-differences.html#planned-contrasts" id="toc-planned-contrasts">Planned Contrasts</a></li>
<li><a href="finding-group-differences.html#additional-practice-5" id="toc-additional-practice-5">Additional practice</a></li>
</ul></li>
<li><a href="repeated-measures-anova.html#repeated-measures-anova" id="toc-repeated-measures-anova">17. Repeated Measures ANOVA</a>
<ul>
<li><a href="repeated-measures-anova.html#step-1-look-at-the-data-7" id="toc-step-1-look-at-the-data-7">Step 1: Look at the data</a></li>
<li><a href="repeated-measures-anova.html#step-2-check-assumptions-6" id="toc-step-2-check-assumptions-6">Step 2: Check Assumptions</a></li>
<li><a href="repeated-measures-anova.html#step-3-perform-the-test-7" id="toc-step-3-perform-the-test-7">Step 3: Perform the test</a></li>
<li><a href="repeated-measures-anova.html#step-4-interpret-results-7" id="toc-step-4-interpret-results-7">Step 4: Interpret results</a></li>
<li><a href="repeated-measures-anova.html#friedmans-test" id="toc-friedmans-test">Friedman’s test</a></li>
<li><a href="repeated-measures-anova.html#additional-practice-6" id="toc-additional-practice-6">Additional practice</a></li>
</ul></li>
<li><a href="factorial-anova.html#factorial-anova" id="toc-factorial-anova">18. Factorial ANOVA</a>
<ul>
<li><a href="factorial-anova.html#independent-factorial-anova" id="toc-independent-factorial-anova">Independent Factorial ANOVA</a></li>
<li><a href="factorial-anova.html#repeated-measures-factorial-anova" id="toc-repeated-measures-factorial-anova">Repeated Measures Factorial ANOVA</a></li>
<li><a href="factorial-anova.html#mixed-factorial-anova" id="toc-mixed-factorial-anova">Mixed Factorial ANOVA</a></li>
</ul></li>
<li><a href="ancova.html#ancova" id="toc-ancova">19. ANCOVA</a>
<ul>
<li><a href="ancova.html#step-1-look-at-the-data-8" id="toc-step-1-look-at-the-data-8">Step 1: Look at the data</a></li>
<li><a href="ancova.html#step-2-check-assumptions-7" id="toc-step-2-check-assumptions-7">Step 2: Check Assumptions</a></li>
<li><a href="ancova.html#step-3-perform-the-test-8" id="toc-step-3-perform-the-test-8">Step 3: Perform the test</a></li>
<li><a href="ancova.html#step-3-interpret-results" id="toc-step-3-interpret-results">Step 3: Interpret results</a></li>
</ul></li>
</ul></li>
<li><a href="correlation-and-regression.html#correlation-and-regression" id="toc-correlation-and-regression">Correlation and regression</a>
<ul>
<li><a href="correlation.html#correlation" id="toc-correlation">20. Correlation</a>
<ul>
<li><a href="correlation.html#step-1-look-at-the-data-9" id="toc-step-1-look-at-the-data-9">Step 1: Look at the data</a></li>
<li><a href="correlation.html#step-2-check-assumptions-8" id="toc-step-2-check-assumptions-8">Step 2: Check assumptions</a></li>
<li><a href="correlation.html#step-3-perform-the-test-9" id="toc-step-3-perform-the-test-9">Step 3: Perform the test</a></li>
<li><a href="correlation.html#step-4-interpreting-results" id="toc-step-4-interpreting-results">Step 4: Interpreting results</a></li>
<li><a href="correlation.html#comparing-strengths-of-correlations" id="toc-comparing-strengths-of-correlations">Comparing strengths of correlations</a></li>
<li><a href="correlation.html#additional-practice-7" id="toc-additional-practice-7">Additional practice</a></li>
</ul></li>
<li><a href="regression.html#regression" id="toc-regression">21. Regression</a>
<ul>
<li><a href="regression.html#step-1-look-at-the-data-10" id="toc-step-1-look-at-the-data-10">Step 1: Look at the data</a></li>
<li><a href="regression.html#step-2-check-assumptions-9" id="toc-step-2-check-assumptions-9">Step 2: Check Assumptions</a></li>
<li><a href="regression.html#step-3-perform-the-test-10" id="toc-step-3-perform-the-test-10">Step 3: Perform the test</a></li>
<li><a href="regression.html#step-4-interpret-results-8" id="toc-step-4-interpret-results-8">Step 4: Interpret results</a></li>
<li><a href="regression.html#categorical-predictors" id="toc-categorical-predictors">Categorical Predictors</a></li>
<li><a href="regression.html#hierarchical-regression" id="toc-hierarchical-regression">Hierarchical regression</a></li>
<li><a href="regression.html#additional-practice-8" id="toc-additional-practice-8">Additional practice</a></li>
</ul></li>
<li><a href="general-linear-model.html#general-linear-model" id="toc-general-linear-model">22. General Linear Model</a>
<ul>
<li><a href="general-linear-model.html#correlation-as-a-regression" id="toc-correlation-as-a-regression">Correlation as a regression</a></li>
<li><a href="general-linear-model.html#independent-t-test-as-a-regression" id="toc-independent-t-test-as-a-regression">Independent t-test as a regression</a></li>
<li><a href="general-linear-model.html#dependent-t-test-as-a-regression" id="toc-dependent-t-test-as-a-regression">Dependent t-test as a regression</a></li>
<li><a href="general-linear-model.html#one-way-anova-as-a-regression" id="toc-one-way-anova-as-a-regression">One-way ANOVA as a regression</a></li>
</ul></li>
</ul></li>
<li><a href="#appendix-appendices" id="toc-appendix-appendices">(APPENDIX) Appendices</a></li>
<li><a href="answers-to-your-turn-exercises.html#answers-to-your-turn-exercises" id="toc-answers-to-your-turn-exercises">Answers to Your Turn exercises</a></li>
<li class="divider"></li>
<li><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/" 
    target="blank"><img alt="Creative Commons License" 
    style="border-width:0" 
    src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics with jamovi</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level2">
<h2>21. Regression</h2>
<p>Regression can examine multiple predictor variables simultaneously. Whereas the factorial ANOVA can only handle categorical variables (i.e., nominal or ordinal), regression can handle all types of predictor variables including both categorical and continuous.</p>
<p>There are three types of regression in general:</p>
<ol style="list-style-type: decimal">
<li><p><u>Linear regression</u>: this looks at the effect of a single predictor (IV) on a single outcome (DV). This is equivalent to a t-test (dichotomous predictor), one-way ANOVA (ordinal predictor), or correlation (scale predictor).</p></li>
<li><p><u>Multiple regression</u>: this looks at the effect of multiple predictors (IVs) on a single outcome (DV).</p></li>
<li><p><u>Hierarchical regression</u>: this looks at the effect of multiple predictors (IVs) on a single outcome (DV), but there are multiple “blocks” or “steps” so that you can check the added predictability of new variables.</p></li>
</ol>
<p>Note that the linear regression is actually equivalent to a lot of the statistics we’ve learned. For example, the linear regression will produce the same results as a t-test when we have a dichotomous predictor, a one-way ANOVA when we have an ordinal predictor, and a correlation when we have a continuous predictor. We’ll learn more about this at the end of the textbook when we wrap everything up.</p>
<div id="understanding-regression" class="section level4">
<h4>Understanding regression</h4>
<p>A linear regression model is basically a linear line, which many of us learned as y = mx + b, where y is our predicted outcome score, x is the IV, b is the intercept (the score in y when x = 0), and m is the slope (when you increase x-value by 1 unit, the y-value goes up by m units).</p>
<p>Let’s imagine we have a dataset of dragons with a categorical predictor (whether they are spotted or striped) and a continuous predictor (height) and a continuous dependent variable (weight). We want to use this dataset to be able to predict the weight of future dragons. First, let’s learn how to interpret the coefficients for our two predictor variables (images from <a href="https://github.com/allisonhorst/stats-illustrations#multiple-linear-regression-dragons-thread">Allison Horst</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="images/13-regression/dragon_categorical.png" alt="Regression lines and residuals" width="49%" /><img src="images/13-regression/dragons_continuous.png" alt="Regression lines and residuals" width="49%" />
<p class="caption">
Figure 2: Regression lines and residuals
</p>
</div>
<p>We determine our line equation from the scatterplot of scores by figuring out the line that fits closest to all data points. The regression line is the line with the <em>smallest</em> residuals between the line and data points.</p>
<p>Let’s visualize the regression line for how Dan’s sleepiness affect Dan’s grumpiness. On the left, we see the regression line (in purple) is very close to the data points and the residuals (the grey lines between the purple line and the data points) are smaller. On the right, we see the regression line is far from a lot of the data points and the residuals are larger.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="images/13-regression/good-regression-line.jpg" alt="Regression lines and residuals" width="49%" /><img src="images/13-regression/bad-regression-line.jpg" alt="Regression lines and residuals" width="49%" />
<p class="caption">
Figure 3: Regression lines and residuals
</p>
</div>
<p>Let’s go back to our dragon example and input one of our dragons into the model to find out how residuals work. On the left, based on our dataset and the fact that our dragon is striped (spotted = 0) and has a height of 5.1 feet, we would expect our dragon to weigh 3.9 tons. However, when we actually weigh him, he weighs 4.2 tons! Therefore, the residual in this case would be .3 tons.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="images/13-regression/dragon_predict_mlr.png" alt="Regression lines and residuals" width="49%" /><img src="images/13-regression/dragon_residual.png" alt="Regression lines and residuals" width="49%" />
<p class="caption">
Figure 4: Regression lines and residuals
</p>
</div>
<p>One of our assumption checks is that our residuals are normally distributed, so we would take all our residuals and examine those for normality.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="images/13-regression/dragon_residual_distribution.png" alt="Regression lines and residuals" width="49%" />
<p class="caption">
Figure 5: Regression lines and residuals
</p>
</div>
<p>There is more math to regression, which is needed to calculate the <em>F</em>-test you get for the overall model test and the t-tests you get for your model coefficients, but we won’t get into that detail.</p>
</div>
<div id="step-1-look-at-the-data-10" class="section level3">
<h3>Step 1: Look at the data</h3>
<p>Let’s run an example with data from lsj-data. Open data from your Data Library in “lsj-data”. Select and open <code>parenthood</code>. This dataset includes the sleep quality of both Dan and Dan’s baby, Dan’s grumpiness, and the day of the data collection from 1-100.</p>
<p>We’ll be testing how both Dan’s and Dan’s baby’s quality of sleep affect Dan’s grumpiness.</p>
<p>Here’s a video walking through the regression.</p>
<div class="vembedr">
<div>
<iframe src="https://www.youtube.com/embed/SfubvsatkdA" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
<div id="data-set-up-9" class="section level4">
<h4>Data set-up</h4>
<p>Our data set-up for regression depends on the type of regression and type of data, but in general we’ll have one column of our continuous DV and one or more columns of our IV(s).</p>
<p>For this chapter, we’re going to return to the <code>parenthood</code> dataset from lsj-data. Remember that this dataset includes the sleep quality of both Dan and Dan’s baby, Dan’s grumpiness, and the day of the data collection from 1-100.</p>
<p><img src="images/08-correlation/correlation-data.png" /></p>
</div>
<div id="describe-the-data-8" class="section level4">
<h4>Describe the data</h4>
<p>Once we confirm our data is setup correctly in jamovi, we should look at our data using descriptive statistics and graphs. First, our descriptive statistics are shown below. We can see first that we have 100 cases and no missing data. The means, medians, standard deviations, and variances are then shown, followed by the minimum and maximum values.</p>
<p>We also see skew and kurtosis. Calculating the <em>z</em>-score for all the skew and kurtosis (remember: skew or kurtosis divided by its standard error) suggests we do not violate the assumption of normality much except for <code>day</code>. However, notice what the variable <code>day</code> is! It’s just the day of the study, from 1-100. If you look at the graph, it has a <em>uniform distribution</em> (completely flat and uniform) not a normal distribution (bell curve)!</p>
<p><img src="images/08-correlation/correlation-descriptives.png" /></p>
</div>
</div>
<div id="step-2-check-assumptions-9" class="section level3">
<h3>Step 2: Check Assumptions</h3>
<p>The regression has a lot of assumptions.</p>
<p>Some require no testing and are a function of understanding your data and research design:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Variable types</strong>: The DV is continuous and the IVs are either continuous, dichotomous, or ordinal.</p></li>
<li><p><strong>Independence</strong>: All the outcome variable values are independent (e.g., come from a separate entity).</p></li>
</ol>
<p>Other assumptions require testing. We’ll go through each of these one at a time.</p>
<div id="no-outliers" class="section level4">
<h4>No outliers</h4>
<p>There shouldn’t be any data in the dataset that is an outlier which would strongly influence your results.</p>
<p>Under Data Summary, you should have a table with Cook’s distance. This is one way we can check for <em>multivariate outliers</em>. This examines whether any one <em>line</em> of data is an outlier, not just one data <em>point</em>. In general, Cook’s distances greater than 1 indicate a multivariate outlier. Our Cook’s distances are very small, so we do not have a problem with outliers.</p>
<p><img src="images/13-regression/regression-cooks.png" /></p>
<p>If you violate this assumption and have one or more multivariate outliers, then go to the Save tab in your regression setup and select the box for Cook’s distance. That will create a new variable in your dataset with the Cook’s distance. Create a filter and remove any data that has a Cook’s distance greater than 1.</p>
</div>
<div id="normality-of-the-residuals" class="section level4">
<h4>Normality of the residuals</h4>
<p>Up to this point, we’ve examined the normality of the outcome variables. With regression, our variables can be non-normal as long as the residuals (i.e., error) are normally distributed.</p>
<p>The regression analysis in jamovi allows us to check multivariate normality with the Shapiro-Wilk’s test and the Q-Q plot of our residuals. We’ve seen this multiple times, so by now it should be well-ingrained that because Shapiro-Wilk’s is not statistically significant and our data points fall along the diagonal line that we satisfy the assumption of normally distributed residuals.</p>
<p>Note that in large datasets, your Q-Q plot may look fine but your Shapiro-Wilk’s test may be statistically significant. In that case, you can likely assume your data is normally distributed. The Shapiro-Wilk’s test tends to say things are not normal with very small deviations from large datasets.</p>
<p>If you violate the assumption of normality, you can try transformating one or more variables. For the purposes of our assignments, if you violate this assumption then just mention that it is violated and that you will proceed anyways.</p>
<p><img src="images/13-regression/regression-normality.png" /></p>
</div>
<div id="linearity-homoscedasticity" class="section level4">
<h4>Linearity &amp; homoscedasticity</h4>
<p><strong>Linearity</strong>: The assumption of linearity means relationship between each IV and DV is linear. Sometimes you may expect a curvilinear relationship between an IV and DV, in which case we square or cube the IV and use that variable as our predictor variable in the regression.</p>
<p><strong>Homogeneity of variance (homoscedasticity)</strong>: The assumption of homoscedasticity means at each level of the predictor variables, the variance of the residual terms should be constant.</p>
<p>To examine linearity and homoscedasticity we examine the Residuals Plots. You will get one plot of the overall model (Fitted) and one for each of your variables (DV and IV(s). We only focus on the Fitted residuals, shown below. In these plots, we want our data to look like a random scattering of dots even dispersed around zero on the y-axis.</p>
<p>We will not be learning non-normal or weighted least squares regression in this course, so if you violate one or both of these assumptions then mention which are violated and that you will proceed anyways.</p>
<p><img src="images/13-regression/regression-residuals.png" /></p>
<p><strong>Linearity</strong>: If the data points seem to have a curve in the graph, then that suggests you have failed the assumption of linearity. Our data doesn’t seem to have any curve to it, so we satisfy the assumption of linearity.</p>
<p>This image by <a href="https://statistics.laerd.com/spss-tutorials/pearsons-product-moment-correlation-using-spss-statistics.php">Laerd Statistics</a> shows what non-linear relationships might look like.</p>
<p><img src="images/08-correlation/linear-non-linear.png" /></p>
<p><strong>Homoscedasticity</strong>: If the graph seems to funnel (e.g., widely dispersed on one end of the x-axis and narrowly dispersed on the other end), then that suggests you fail the assumption of homoscedasticity. Our data doesn’t seem to be wider at any point, so we satisfy the assumption of homoscedasticity.</p>
<p>This image by <a href="https://clevertap.com/blog/a-brief-primer-on-linear-regression-part-ii/">CleverTap</a> shows what heteroscedasticity might look like.</p>
<p><img src="images/08-correlation/homoscedasticity.png" /></p>
</div>
<div id="independence-of-residuals" class="section level4">
<h4>Independence of residuals</h4>
<p>This assumption means that for any two observations, the residual terms should be uncorrelated (or independent). Our errors must be normally distributed and uncorrelated.</p>
<p>The Durbin-Watson test for autocorrelation tests for independence of residuals. We want the Durbin-Watson value to be as close to 2 as possible. Values less than 1 or greater than 3 are problematic and indicate we are violating this assumption. In our case, the DW test statistic is 2.12 and so very close to 2. Furthermore, they provide a p-value and the p-value is greater than .05 so the test statistic is not statistically significant, further supporting that we meet the assumption that our residuals are independent.</p>
<p>If you violate this assumption, it’s likely a function of how your data was collected (e.g., a time effect or you have nested data). We won’t be covering what to do in these cases, but if you have nested data you may be interested in multilevel or hierarchical modeling (MLM/HLM).</p>
<p><img src="images/13-regression/regression-durbinwatson.png" /></p>
</div>
<div id="no-multicollinearity" class="section level4">
<h4>No multicollinearity</h4>
<p>This assumption means there should be no perfect or near-perfect linear relationship between two or more of the predictors in your regression model. For example, you would not include “heigh_cm” and “heigh_in” in your model because they would be perfectly related to one another.</p>
<p>Multicollinearity is a problem for three reasons:</p>
<ol style="list-style-type: decimal">
<li><strong>Untrustworthy <em>B</em>s</strong>: As multicollinearity increases, so do the standard errors of the <em>B</em> coefficient. We want smaller standard errors, so this is problematic.</li>
<li><strong>Limits the size of R</strong>, and therefore the size of R<sup>2</sup>, and we want to have the largest R or R<sup>2</sup> possible, given our data.</li>
<li><strong>Importance of predictors</strong>: When two predictors are highly correlated, it is very hard to determine which variable is more important than the other.</li>
</ol>
<p>Multicollinearity is simply that multiple variables are correlated. We can first just look for general <em>collinearity</em>, or the correlations between all our predictors, using the correlation matrix in jamovi. Any correlations greater than .8 or .9 are problematic. You would either need to drop one variable or combine them into a mean composite variable.</p>
<p>However, to test for <em>multicollinearity</em>, we examine the VIF and Tolerance values. VIF is actually a transformation of Tolerance (Tolerance = 1/VIF and VIF = 1/Tolerance). In general, we want values 10 or lower, which corresponds to Tolerance values greater than .2.</p>
<p>In our data, our VIF is 1.65 and Tolerance is .61, so we satisfy the assumption of no multicollinearity.</p>
<p><img src="images/13-regression/regression-multicollinearity.png" /></p>
<p>Now that we met all the assumptions, we can interpret our results!</p>
</div>
</div>
<div id="step-3-perform-the-test-10" class="section level3">
<h3>Step 3: Perform the test</h3>
<ol style="list-style-type: decimal">
<li><p>From the ‘Analyses’ toolbar select ‘Regression’ - ‘Linear regression’. Note that we select this option regardless of whether we are performing a linear regression, multiple regression, or hierarchical regression.</p></li>
<li><p>Move your dependent variable <code>dan.grump</code> into the Dependent Variable box and all your independent variables into either Covariates (if they are continuous variables) or Factors (if they are categorical variables). In this case, all our variables are continuous so move both <code>dan.sleep</code> and <code>baby.sleep</code> to the Covariates box.</p></li>
<li><p>If you are performing a hierarchical regression, you will use the Model Builder drop-down menu. More information on hierarchical regression will be discussed later.</p></li>
<li><p>If you have categorical predictors with more than two levels, you will use the Reference Levels drop-down menu to specify what you want your reference level to be and whether you want the intercept to be the reference level or the grand man. More information on categorical predictors will be discussed later.</p></li>
<li><p>Under Assumption Checks, check <em>all</em> the boxes!</p></li>
<li><p>Under Model Fit, select <code>R</code>, <code>R-squared</code>, <code>Adjusted R-squared</code>, and <code>F test</code>. The other options (AIC, BIC, RMSE) are more useful when we are comparing models and will be discussed later in the Hierarchical regression section.</p></li>
<li><p>Under Model Coefficients, select <code>Standardized estimate</code>.</p></li>
<li><p>Optionally, you can ask for plots and tables of the estimated marginal means.</p></li>
</ol>
<p>I’m not going to show the set-up figure here because there’s just too much to show.</p>
</div>
<div id="step-4-interpret-results-8" class="section level3">
<h3>Step 4: Interpret results</h3>
<p><img src="images/13-regression/regression-results.png" /></p>
<p>The first table shows us our overall model results.</p>
<p><strong>R, R-squared, and adjusted R-squared</strong>: We get our R and R-squared values (R-squared literally being R squared). Remember back to correlation: R-squared is the <em>proportion</em> of variance in the dependent variable that can be accounted for by the predictor(s). In this case, Dan and the baby’s sleep quality predict 82% of the variance in Dan’s grumpiness.</p>
<p>However, more commonly we report the adjusted R-squared value, which adjusts the R-squared value based on the number of predictors in the model. Adding more predictors to the model will <em>always</em> cause R-squared to increase (or at least not decrease) so it’s important that we control for that using an adjustment. It’s interpreted basically the same, just adjusted for biased. I encourage you to use the adjusted R-squared, <em>especially</em> if you have lots of predictors in your model.</p>
<p><strong>Overall Model Test</strong>: We also get an <em>F</em>-test for the overall model. If you want, you can get the full ANOVA test by selected ANOVA test under Model Coefficients. This is how we know if the overall model is statistically significant. In our case, our <em>F</em>-test is statistically significant so we know that the set of predictors significantly predicts our dependent variable.</p>
<p><strong>Model coefficients</strong>: Just like in ANOVA, we first examine if the model is significant (overall model test) and then look at individual factors, in this case being individual variables in our regression model. Each variable–our intercept and both independent variables–have an associated <em>t</em>-test. In this case, Dan’s sleep significantly predicts Dan’s grumpiness, but the baby’s sleep does not.</p>
<p><strong>Standardized coefficients</strong>: We also asked for standardized estimates (Stand. Estimate), which we get in the last column of our model coefficients table. These are <em>standardized</em> so that we can compare them to other variables. They give us an idea of the <em>strength</em> of the relationship between that IV on the DV. Larger values = bigger effects. The standardized estimate is called Beta (<span class="math inline">\(\beta\)</span>) whereas the unstandardized estimate is just called that or B (the letter B, not Beta). We use the standardized estimates to compare the strength of the estimate to other IVs and we use unstandardized estimates to write our linear equations and predict the DV given values of the IV.</p>
<p><strong><em>What about the intercept?</em></strong>You might be wondering what we do with the intercept. Typically, nothing. We only use it to create our equation so that we can predict Dan’s grumpiness based on Dan’s sleep and the baby’s sleep. For example, our equation from our data is such:</p>
<p><span class="math inline">\(y = 125.97 - 8.95(dan.sleep) + .01(baby.sleep)\)</span></p>
<p>If Dan’s sleep was 5 and baby’s sleep was 8, then we’d expect Dan’s grumpiness to be:</p>
<p><span class="math inline">\(y = 125.97 - 8.95(5) + .01(8) = 125.97 - 44.75 + .08 = 81.3\)</span></p>
<div id="write-up-the-results-in-apa-style-10" class="section level4">
<h4>Write up the results in APA style</h4>
<p>We can write up our results in APA something like this:</p>
<blockquote>
<p>Dan collected data on how many hours of sleep Dan and Dan’s baby got, as well as Dan’s grumpiness, for 100 days. Dan tested how the hours of sleep both Dan and the baby got affected Dan’s grumpiness using a multiple regression. The combination of predictors were significantly related to Dan’s grumpiness, <em>F</em> (2, 97) = 215.24, <em>p</em> &lt; .001, adjusted <span class="math inline">\(R^2\)</span> = .81. The number of hours of sleep Dan got significantly predicted Dan’s grumpiness, <span class="math inline">\(\beta\)</span> = -.90, <em>t</em> (97)= -16.17, <em>SE</em> = .55, <em>p</em> &lt; .001. However, the number of hours of sleep the baby got did not significantly relate to Dan’s grumpiness, <span class="math inline">\(\beta\)</span> = 0.00, <em>t</em> (97)= .04, <em>SE</em> = .27, <em>p</em> = .969.</p>
</blockquote>
<p>Note that in many of these write-ups I did not include anything about assumption checking. I normally write up that information as part of my analytic plan in my methods section (e.g., “I checked for multivariate outliers using Cook’s distance.”). Included in this section, I explain what I will do if I do not meet various assumptions. Then, if I don’t meet the assumption in the results section I explain that I did not meet the assumption, explain the results if necessary, explain what I did, and then give the results. In this case, we met all the assumptions (that presumably I described in my methods section) and therefore went straight to the results.</p>
</div>
</div>
<div id="categorical-predictors" class="section level3">
<h3>Categorical Predictors</h3>
<p>Dummy coded variables (with values 0 or 1) are pretty easy to interpret in regression. If the Beta is positive, then the value of 1 would have a higher mean on the DV than the value of 0. If the Beta is negative, then the value of 0 would have a higher mean on the DV than the value of 1.</p>
<p>However, if we have a nominal variable with more than two categories, then we need to dummy code the data to analyze in a regression. Fortunately, jamovi can do this automatically for us!</p>
<p>The dataset we’re using doesn’t currently have a categorical variable, so I’m going to manually create one for demonstration purposes. I’m going to transform the <code>day</code> variable, which is the day of the data collection from 1 to 100, into a new variable that indicates whether the day is 1-32, 33-65, or 66-100, which is roughly 3 equal groups. You can see the transformation here:</p>
<p><img src="images/13-regression/transform.png" /></p>
<p>Let’s add that to our regression model and just make it a simple multiple regression model. Three independent variables (<code>dan.sleep</code>, <code>baby.sleep</code>, and our new <code>day_3groups</code> variable) all in one block.</p>
<p>Now we need to go to the Reference Levels drop-down menu. We have two options:</p>
<ol style="list-style-type: decimal">
<li><strong>Reference level (dummy coding)</strong>: We can have our intercept be the mean of our reference level group, meaning that if all other variables were set to 0 this is the mean of our dependent variable for that group. For example, if we set day = 1 to be our reference level, then the intercept is the value of Dan’s grumpiness when Dan’s sleep is 0 and baby’s sleep is 0 for the first 32 days. <em>This is the option I normally choose.</em></li>
<li><strong>Grand mean (simple coding)</strong>: Alternatively, we can have our intercept be the grand mean, or the overall mean when all other variables were set to 0 and we ignored day. <em>I am not sure when I would use this option, to be honest</em>.</li>
</ol>
<p>The other option you have is what is considered your reference level. It will default to your first level in your dataset (in this case, day_3Groups = 1) but you can change to any other level in your variable. I set my reference level to be 1, the default, and I know that because the day variable compares both levels 2 and 3 to 1. Our intercept (126.02) then is the value of grumpiness if Dan and the baby slept 0 hours in the first 32 days of our data collection.</p>
<p><img src="images/13-regression/regression-categorical.png" /></p>
<p>The first line of day - 3_Groups (2 – 1) is then the difference in Dan’s grumpiness between the second 1/3 of days (days 33-65) and the first 1/3 of days (days 1-32). It is not statistically significant, so there is no difference in Dan’s grumpiness between the first and second 1/3 of days. Because the estimate is negative, that indicates that the first 1/3 of days have a higher estimated mean of Dan’s grumpiness, but again it’s not statistically significant.</p>
<p>The second line of day - 3_Groups (3 – 1) is the difference in Dan’s grumpiness between the third 1/3 of days (days 66-100) and the first 1/3 of days (days 1-32). It is not statistically significant, so there is no difference in Dan’s grumpiness between the first and third 1/3 of days, either.</p>
<p>In this case, the Estimated Marginal Means can be very helpful for us to interpret the model coefficients. We can get the estimated marginal means of each group on the DV at the average levels of the other two variables.</p>
<p><img src="images/13-regression/regression-categorical-EMM.png" /></p>
</div>
<div id="hierarchical-regression" class="section level3">
<h3>Hierarchical regression</h3>
<p>Hierarchical regression is exactly the same as multiple regression but now we have multiple models or blocks. You can specify hierarchical regression using the Model Builder drop-down menu in jamovi. Let’s try an example where we have <code>baby.sleep</code> as Block 1 and <code>dan.sleep</code> as Block 2. In addition, using the Model Fit drop-down menu you should check <code>AIC</code> and <code>BIC</code> in addition to the previously selected options. Your setup should look something like this:</p>
<p><img src="images/13-regression/hierarchical-setup.png" /></p>
<p>Our model results now change. We now have two lines for the Model Fit Measures and a Model Comparisons table. In addition, under Model Specific Results, we have a drop-down menu to specify which model we want to examine.</p>
<p><img src="images/13-regression/hierarchical-results.png" /></p>
<p>Let’s interpret. Our first model (with just <code>baby.sleep</code> is significant), <em>F</em> (1, 98) = 46.18, <em>p</em> &lt; .001, <em>adjusted</em> <span class="math inline">\(R^2\)</span> = .31. So is our second model (that has both <code>baby.sleep</code> and <code>dan.sleep</code>), <em>F</em> (2, 97) = 215.24, <em>p</em> &lt; .001, <em>adjusted</em> <span class="math inline">\(R^2\)</span> = .81. There was a significant improvement between model 1 and model 2, <span class="math inline">\(F_{change}\)</span> (1, 97) = 261.52, <em>p</em> &lt; .001, <span class="math inline">\(\Delta R^2\)</span> = .50. The significant improvement means that the predictors added to model 2 significantly predict our DV <em>above and beyond</em> the predictors in model 1.</p>
<p>We might write-up these results as such:</p>
<blockquote>
<p>Dan collected data on how many hours of sleep Dan and Dan’s baby got, as well as Dan’s grumpiness, for 100 days. Dan tested how the hours of sleep both Dan and the baby got affected Dan’s grumpiness using hierarchical regression to find out how Dan’s sleep predicted Dan’s grumpiness above and beyond the baby’s sleep.</p>
<p>First, the baby’s sleep significantly predicted Dan’s grumpiness<em>, F</em> (1, 98) = 46.18, <em>p</em> &lt; .001, <em>adjusted</em> <span class="math inline">\(R^2\)</span> = .31. As the baby’s hours of sleep increased, Dan’s grumpiness decreased, <em>t</em> (98) = -6.80, <em>SE</em> = .40, <em>p</em> &lt; .001, <span class="math inline">\(\beta\)</span> = -.57.</p>
<p>A second model was tested that added Dan’s sleep. This model–comprised of both baby’s sleep and Dan’s sleep–significantly predicted Dan’s grumpiness, <em>F</em> (2, 97) = 215.24, <em>p</em> &lt; .001, <em>adjusted</em> <span class="math inline">\(R^2\)</span> = .81. There was a significant improvement between model 1 and model 2, <span class="math inline">\(F_{change}\)</span> (1, 97) = 261.52, <em>p</em> &lt; .001, <span class="math inline">\(\Delta R^2\)</span> = .50. In the second model, as Dan’s sleep increased, Dan’s grumpiness decreased, <em>t</em> (97) = -16.17, <em>SE</em> = .55, <em>p</em> &lt; .001, <span class="math inline">\(\beta\)</span> = -.90. However, the baby’s sleep did not significantly relate to Dan’s grumpiness when controlling for Dan’s sleep, <em>t</em> (97) = .04, <em>SE</em> = .27, <em>p</em> = .969, <span class="math inline">\(\beta\)</span> = .00.</p>
</blockquote>
</div>
<div id="additional-practice-8" class="section level3">
<h3>Additional practice</h3>
<p>Open the <code>Sample_Dataset_2014.xlsx</code> file that we will be using for all Your Turn exercises. You can find the dataset here: <a href="https://github.com/danawanzer/stats-with-jamovi/blob/master/data/Sample_Dataset_2014.xlsx">Sample_Dataset_2014.xlsx Download</a></p>
<p>To get the most out of these exercises, try to first find out the answer on your own and then use the drop-down menus to check your answer.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Perform a multiple regression examining how <code>English</code></strong>, <strong><code>Reading</code> and <code>Writing</code>, as well as <code>Gender</code> relate to the dependent variable <code>Math</code>.</strong></p>
<ul>
<li><p>Do you have any significant outliers?</p></li>
<li><p>Are your residuals normally distributed?</p></li>
<li><p>Do you satisfy the assumption of linearity and homoscedasticity of your residuals (just check the Fitted residual plot)?</p></li>
<li><p>Do you meet the assumption of independent residuals?</p></li>
<li><p>Do you meet the assumption of no multicollinearity?</p></li>
<li><p>Can you perform a regression with this data?</p></li>
<li><p>What is your adjusted R-squared, rounded to two decimal places?</p></li>
<li><p>Is the overall model statistically significant?</p></li>
<li><p>Is <code>English</code> statistically significant?</p></li>
<li><p>Is <code>Reading</code> statistically significant?</p></li>
<li><p>Is <code>Writing</code> statistically significant?</p></li>
<li><p>Is <code>Gender</code> statistically significant?</p></li>
<li><p>For <code>Gender</code>, do male (Gender = 0) or female (Gender = 1) students have higher math scores?</p></li>
</ul></li>
</ol>

</div>
</div>
<div class="psyteachr_footer">
  
</div>
<script>

/* update total correct if #total_correct exists */
update_total_correct = function() {
  if (t = document.getElementById("total_correct")) {
    t.innerHTML =
      document.getElementsByClassName("correct").length + " of " +
      document.getElementsByClassName("solveme").length + " correct";
  }
}

/* solution button toggling function */
b_func = function() {
  var cl = this.parentElement.classList;
  if (cl.contains('open')) {
    cl.remove("open");
  } else {
    cl.add("open");
  }
}

/* function for checking solveme answers */
solveme_func = function(e) {
  var real_answers = JSON.parse(this.dataset.answer);
  var my_answer = this.value;
  var cl = this.classList;
  if (cl.contains("ignorecase")) {
    my_answer = my_answer.toLowerCase();
  }
  if (cl.contains("nospaces")) {
    my_answer = my_answer.replace(/ /g, "");
  }
  
  if (my_answer !== "" & real_answers.includes(my_answer)) {
    cl.add("correct");
  } else {
    cl.remove("correct");
  }
  update_total_correct();
}

window.onload = function() {
  /* set up solution buttons */
  var buttons = document.getElementsByTagName("button");

  for (var i = 0; i < buttons.length; i++) {
    if (buttons[i].parentElement.classList.contains('solution')) {
      buttons[i].onclick = b_func;
    }
  }
  
  /* set up solveme inputs */
  var solveme = document.getElementsByClassName("solveme");

  for (var i = 0; i < solveme.length; i++) {
    /* make sure input boxes don't auto-anything */
    solveme[i].setAttribute("autocomplete","off");
    solveme[i].setAttribute("autocorrect", "off");
    solveme[i].setAttribute("autocapitalize", "off"); 
    solveme[i].setAttribute("spellcheck", "false");
    solveme[i].value = "";
    
    /* adjust answer for ignorecase or nospaces */
    var cl = solveme[i].classList;
    var real_answer = solveme[i].dataset.answer;
    if (cl.contains("ignorecase")) {
      real_answer = real_answer.toLowerCase();
    }
    if (cl.contains("nospaces")) {
      real_answer = real_answer.replace(/ /g, "");
    }
    solveme[i].dataset.answer = real_answer;
    
    /* attach checking function */
    solveme[i].onkeyup = solveme_func;
    solveme[i].onchange = solveme_func;
  }
  
  update_total_correct();
}

</script>
            </section>

          </div>
        </div>
      </div>
<a href="correlation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="general-linear-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"],
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
